{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa940193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/kavyansh/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return _C._get_float32_matmul_precision()\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ AEGNN BASELINE TRAINING\n",
      "================================================================================\n",
      "üì¶ Model: graph_res\n",
      "üìä Dataset: N-Caltech101\n",
      "üî¢ Batch Size: 8\n",
      "üìà Epochs: 100\n",
      "‚öôÔ∏è  Learning Rate: 0.001\n",
      "üéØ Events/Sample: 15000\n",
      "üìê Radius: 3.0\n",
      "üîó Max Neighbors: 32\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Model parameters: 20,401,120\n",
      "\n",
      "üèÉ Starting training...\n",
      "\n",
      "\n",
      "üì¶ Loading AEGNN dataset...\n",
      "\n",
      "======================================================================\n",
      "üöÄ Creating AEGNN DataLoaders\n",
      "======================================================================\n",
      "Batch size: 8\n",
      "Num workers: 0\n",
      "Debug mode: True\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory not found: datasets/ncaltech/img",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 328\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 312\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müèÉ Starting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Running final evaluation...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:584\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:630\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    623\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    624\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    626\u001b[39m     ckpt_path,\n\u001b[32m    627\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    628\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    629\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1039\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path, weights_only)\u001b[39m\n\u001b[32m   1036\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: preparing data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1037\u001b[39m \u001b[38;5;28mself\u001b[39m._data_connector.prepare_data()\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_setup_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001b[39;00m\n\u001b[32m   1040\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: configuring model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1041\u001b[39m call._call_configure_model(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:108\u001b[39m, in \u001b[36m_call_setup_hook\u001b[39m\u001b[34m(trainer)\u001b[39m\n\u001b[32m    105\u001b[39m trainer.strategy.barrier(\u001b[33m\"\u001b[39m\u001b[33mpre_setup\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.datamodule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43m_call_lightning_datamodule_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msetup\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m _call_callback_hooks(trainer, \u001b[33m\"\u001b[39m\u001b[33msetup\u001b[39m\u001b[33m\"\u001b[39m, stage=fn)\n\u001b[32m    110\u001b[39m _call_lightning_module_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33msetup\u001b[39m\u001b[33m\"\u001b[39m, stage=fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:199\u001b[39m, in \u001b[36m_call_lightning_datamodule_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.datamodule.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mNCaltech101DataModule.setup\u001b[39m\u001b[34m(self, stage)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create train/test dataloaders.\"\"\"\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müì¶ Loading AEGNN dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28mself\u001b[39m.train_loader, \u001b[38;5;28mself\u001b[39m.test_loader = \u001b[43mget_aegnn_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mroot\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_workers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_max\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43md_max\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_samples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbeta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_ratio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom_seed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Setup complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MySSD/research project/zeigler/sparse_gnn/main/dataset_caltech_aegnn.py:573\u001b[39m, in \u001b[36mget_aegnn_dataloaders\u001b[39m\u001b[34m(root_dir, batch_size, num_workers, r, d_max, n_samples, beta, train_ratio, random_seed, debug)\u001b[39m\n\u001b[32m    570\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    572\u001b[39m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m train_dataset = \u001b[43mNCaltech101AEGNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43md_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m test_dataset = NCaltech101AEGNN(\n\u001b[32m    584\u001b[39m     root_dir=root_dir,\n\u001b[32m    585\u001b[39m     split=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    590\u001b[39m     debug=debug\n\u001b[32m    591\u001b[39m )\n\u001b[32m    593\u001b[39m \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MySSD/research project/zeigler/sparse_gnn/main/dataset_caltech_aegnn.py:389\u001b[39m, in \u001b[36mNCaltech101AEGNN.__init__\u001b[39m\u001b[34m(self, root_dir, split, r, d_max, n_samples, beta, train_ratio, random_seed, debug)\u001b[39m\n\u001b[32m    387\u001b[39m img_dir = \u001b[38;5;28mself\u001b[39m.root_dir / \u001b[33m'\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m img_dir.exists():\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;66;03m# Collect samples\u001b[39;00m\n\u001b[32m    392\u001b[39m all_samples = []\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Directory not found: datasets/ncaltech/img"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "AEGNN BASELINE TRAINING - N-Caltech101\n",
    "================================================================================\n",
    "Official AEGNN reproduction using graph_res network\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add AEGNN to path\n",
    "aegnn_root = Path(\"./aegnn\").resolve()\n",
    "if aegnn_root.exists():\n",
    "    sys.path.insert(0, str(aegnn_root))\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from aegnn.models.recognition import RecognitionModel\n",
    "from dataset_caltech_aegnn import get_aegnn_dataloaders\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'model': {\n",
    "        'network': 'graph_res',        # AEGNN architecture\n",
    "        'dataset': 'ncaltech101',\n",
    "        'num_classes': 101,\n",
    "        'img_shape': (180, 240),\n",
    "        'dim': 3,                      # 3D positions (x, y, t)\n",
    "        'model_kwargs': {}\n",
    "    },\n",
    "    \n",
    "    'dataset': {\n",
    "        'root': './datasets/ncaltech',\n",
    "        'r': 3.0,                      # Spatial radius (paper setting)\n",
    "        'd_max': 32,                   # Max neighbors (paper setting)\n",
    "        'n_samples': 15000,            # Events per sample (paper setting)\n",
    "        'beta': 3e-3,                # Time scaling factor (paper setting)\n",
    "        'train_ratio': 0.7,\n",
    "        'random_seed': 42\n",
    "    },\n",
    "    \n",
    "    'training': {\n",
    "        'batch_size': 8,              # Adjust based on GPU memory\n",
    "        'eval_batch_size': 8,\n",
    "        'num_workers': 0,              # Reduce if CPU bottleneck\n",
    "        'lr': 1e-3,\n",
    "        'weight_decay': 0.0,\n",
    "        'epochs': 100,\n",
    "        'scheduler_step': 30,          # LR decay every 30 epochs\n",
    "        'scheduler_gamma': 0.5,        # LR decay factor\n",
    "    },\n",
    "    \n",
    "    'hardware': {\n",
    "        'gpus': 1,\n",
    "        'accelerator': 'gpu',\n",
    "        'precision': 32,               # Use fp32 for stability\n",
    "    },\n",
    "    \n",
    "    'logging': {\n",
    "        'save_dir': './checkpoints_aegnn',\n",
    "        'log_dir': './runs_aegnn',\n",
    "        'save_top_k': 3,\n",
    "        'log_every_n_steps': 20\n",
    "    },\n",
    "    \n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA MODULE\n",
    "# ============================================================================\n",
    "\n",
    "class NCaltech101DataModule(pl.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning DataModule for AEGNN baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Create train/test dataloaders.\"\"\"\n",
    "        print(\"\\nüì¶ Loading AEGNN dataset...\")\n",
    "        \n",
    "        self.train_loader, self.test_loader = get_aegnn_dataloaders(\n",
    "            root_dir=self.config['dataset']['root'],\n",
    "            batch_size=self.config['training']['batch_size'],\n",
    "            num_workers=self.config['training']['num_workers'],\n",
    "            r=self.config['dataset']['r'],\n",
    "            d_max=self.config['dataset']['d_max'],\n",
    "            n_samples=self.config['dataset']['n_samples'],\n",
    "            beta=self.config['dataset']['beta'],\n",
    "            train_ratio=self.config['dataset']['train_ratio'],\n",
    "            random_seed=self.config['dataset']['random_seed']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Setup complete!\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.test_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTNING MODULE\n",
    "# ============================================================================\n",
    "\n",
    "class AEGNNLightningWrapper(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning wrapper for AEGNN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.save_hyperparameters(config)\n",
    "        \n",
    "        # Initialize AEGNN model\n",
    "        self.model = RecognitionModel(\n",
    "            network=config['model']['network'],\n",
    "            dataset=config['model']['dataset'],\n",
    "            num_classes=config['model']['num_classes'],\n",
    "            img_shape=config['model']['img_shape'],\n",
    "            dim=config['model']['dim'],\n",
    "            **config['model']['model_kwargs']\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # ‚úÖ CRITICAL: Validate AFTER data is on GPU\n",
    "        if self.training or not hasattr(self, '_validated_first_batch'):\n",
    "            print(f\"\\nüîç GPU Validation:\")\n",
    "            print(f\"  Device: {data.x.device}\")\n",
    "            print(f\"  num_nodes: {data.num_nodes}\")\n",
    "            print(f\"  num_edges: {data.num_edges}\")\n",
    "            \n",
    "            if data.num_edges > 0:\n",
    "                max_idx = data.edge_index.max().item()\n",
    "                print(f\"  edge_index.max(): {max_idx}\")\n",
    "                print(f\"  Expected max: {data.num_nodes - 1}\")\n",
    "                \n",
    "                if max_idx >= data.num_nodes:\n",
    "                    print(f\"\\n‚ùå INVALID EDGE_INDEX ON GPU!\")\n",
    "                    print(f\"   Data was valid on CPU but corrupted on GPU\")\n",
    "                    print(f\"   This indicates a CUDA memory corruption bug\")\n",
    "                    \n",
    "                    # Emergency fix\n",
    "                    mask = (data.edge_index[0] < data.num_nodes) & (data.edge_index[1] < data.num_nodes)\n",
    "                    data.edge_index = data.edge_index[:, mask]\n",
    "                    print(f\"   Fixed: new num_edges={data.edge_index.size(1)}\")\n",
    "            \n",
    "            self._validated_first_batch = True\n",
    "        \n",
    "        return self.model(data)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, batch.y)\n",
    "        acc = (logits.argmax(dim=1) == batch.y).float().mean() * 100\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True, batch_size=batch.num_graphs)\n",
    "        self.log('train_acc', acc, prog_bar=True, on_step=False, on_epoch=True, batch_size=batch.num_graphs)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if batch_idx == 0:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"RAW BATCH INSPECTION (before forward):\")\n",
    "            print(f\"  batch.num_nodes: {batch.num_nodes}\")\n",
    "            print(f\"  batch.num_edges: {batch.num_edges}\")\n",
    "            print(f\"  batch.edge_index device: {batch.edge_index.device}\")\n",
    "            print(f\"  batch.edge_index.shape: {batch.edge_index.shape}\")\n",
    "            \n",
    "            if batch.num_edges > 0:\n",
    "                print(f\"  batch.edge_index.max(): {batch.edge_index.max().item()}\")\n",
    "                print(f\"  batch.edge_index.min(): {batch.edge_index.min().item()}\")\n",
    "                \n",
    "                # Check if valid\n",
    "                if batch.edge_index.max() >= batch.num_nodes:\n",
    "                    print(f\"  ‚ùå ALREADY INVALID BEFORE FORWARD!\")\n",
    "                else:\n",
    "                    print(f\"  ‚úì Valid edge_index\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        logits = self(batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, batch.y)\n",
    "        acc = (logits.argmax(dim=1) == batch.y).float().mean() * 100\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, batch_size=batch.num_graphs)\n",
    "        self.log('val_acc', acc, prog_bar=True, batch_size=batch.num_graphs)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "        acc = (logits.argmax(dim=1) == batch.y).float().mean() * 100\n",
    "        \n",
    "        self.log('test_acc', acc, batch_size=batch.num_graphs)\n",
    "        \n",
    "        return {'test_acc': acc}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.config['training']['lr'],\n",
    "            weight_decay=self.config['training']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.config['training']['scheduler_step'],\n",
    "            gamma=self.config['training']['scheduler_gamma']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # Set seed for reproducibility\n",
    "    pl.seed_everything(CONFIG['seed'])\n",
    "    \n",
    "    # Print header\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ AEGNN BASELINE TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üì¶ Model: {CONFIG['model']['network']}\")\n",
    "    print(f\"üìä Dataset: N-Caltech101\")\n",
    "    print(f\"üî¢ Batch Size: {CONFIG['training']['batch_size']}\")\n",
    "    print(f\"üìà Epochs: {CONFIG['training']['epochs']}\")\n",
    "    print(f\"‚öôÔ∏è  Learning Rate: {CONFIG['training']['lr']}\")\n",
    "    print(f\"üéØ Events/Sample: {CONFIG['dataset']['n_samples']}\")\n",
    "    print(f\"üìê Radius: {CONFIG['dataset']['r']}\")\n",
    "    print(f\"üîó Max Neighbors: {CONFIG['dataset']['d_max']}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Create datamodule and model\n",
    "    datamodule = NCaltech101DataModule(CONFIG)\n",
    "    model = AEGNNLightningWrapper(CONFIG)\n",
    "    \n",
    "    # Print model size\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"‚úÖ Model parameters: {params:,}\\n\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(CONFIG['logging']['save_dir'], exist_ok=True)\n",
    "    os.makedirs(CONFIG['logging']['log_dir'], exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=CONFIG['logging']['save_dir'],\n",
    "        filename='aegnn-{epoch:02d}-{val_acc:.2f}',\n",
    "        monitor='val_acc',\n",
    "        mode='max',\n",
    "        save_top_k=CONFIG['logging']['save_top_k'],\n",
    "        save_last=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=30,\n",
    "        mode='max',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Logger\n",
    "    logger = TensorBoardLogger(\n",
    "        CONFIG['logging']['log_dir'],\n",
    "        name='aegnn'\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CONFIG['training']['epochs'],\n",
    "        accelerator=CONFIG['hardware']['accelerator'],\n",
    "        devices=CONFIG['hardware']['gpus'],\n",
    "        precision=CONFIG['hardware']['precision'],\n",
    "        callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=CONFIG['logging']['log_every_n_steps'],\n",
    "#        gradient_clip_val=1.0,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"üèÉ Starting training...\\n\")\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    \n",
    "    # Test\n",
    "    print(\"\\nüìä Running final evaluation...\\n\")\n",
    "    trainer.test(model, datamodule=datamodule, ckpt_path='best')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÅ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Best model: {checkpoint_callback.best_model_path}\")\n",
    "    print(f\"‚úÖ Best val_acc: {checkpoint_callback.best_model_score:.2f}%\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
